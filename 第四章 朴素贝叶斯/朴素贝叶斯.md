# 朴素贝叶斯

[朴素贝叶斯的推导](https://blog.csdn.net/u012162613/article/details/48323777)

[朴素贝叶斯应用](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html)

### 理论

1．朴素贝叶斯法是典型的生成学习方法。生成方法由训练数据学习联合概率分布$P(X,Y)$，然后求得后验概率分布$P(Y|X)$。具体来说，利用训练数据学习$P(X|Y)$和$P(Y)$的估计，得到联合概率分布：
$$
P(X,Y)＝P(Y)P(X|Y)
$$
概率估计方法可以是极大似然估计或贝叶斯估计。

2．朴素贝叶斯法的基本假设是条件独立性，
$$
\begin{aligned} P(X&=x | Y=c_{k} )=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) \end{aligned}
$$

这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。**其缺点是分类的性能不一定很高**。

3．朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测。
$$
P(Y | X)=\frac{P(X, Y)}{P(X)}=\frac{P(Y) P(X | Y)}{\sum_{Y} P(Y) P(X | Y)}
$$
将输入$x$分到后验概率最大的类$y$。
$$
y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X_{j}=x^{(j)} | Y=c_{k}\right)
$$
后验概率最大等价于0-1损失函数时的期望风险最小化。

**补充：**

**(1)** 生成模型和判别模型

**生成方法：**由数据学习联合概率分布$P(X, Y)$,然后求出条件概率分布作为预测的模型，即生成模型：
$$
P(Y | X)=\frac{P(X, Y)}{P(X)}
$$
这样的方法之所以成为生成方法，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。

**判别方法：**由数据直接学习决策函数$f(x)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型，判别方法关心的是对给定的输入$X$,应该预测什么样的$Y$。

### 常见模型

#### 多项式模型

​		在贝叶斯法中，学习意味着估计$P\left(y_{k}\right)$和条件概率$P\left(x_{i} | y_{k}\right)$，而在实际的处理过程中会进行一些平滑处理，保证在某些一般情况下不会存在“０”操作具体公式如下：
$$
P\left(y_{k}\right)=\frac{N_{y_{k}+\alpha}}{N+k \alpha}
$$

$$
P\left(x_{i} | y_{k}\right)=\frac{N_{y_{k}, x_{i}}+\alpha}{N_{y_{k}}+n \alpha}
$$

​		当$ α=1 $$时，称作Laplace平滑，当$ $0<α<1 $时，称作Lidstone平滑，$α=0$时不做平滑。如果不做平滑，当某一维特征的值xi没在训练样本中出现过时，会导致$P(xi|yk)=0P(xi|yk)=0$，从而导致后验概率为0。加上平滑就可以克服这个问题。

具体实例见书**例4.1**

#### 高斯模型

​		当特征是连续变量的时候，运用多项式模型就会导致很多$P(xi|yk)=0$（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。

特征的可能性被假设为高斯

概率密度函数：
$$
P(x_i | y_k)=\frac{1}{\sqrt{2\pi\sigma^2_{yk}}}exp(-\frac{(x_i-\mu_{yk})^2}{2\sigma^2_{yk}})
$$
数学期望(mean)：$\mu$

方差：$\sigma^2=\frac{\sum(X-\mu)^2}{N}$

**代码实现：**

```python
# 1.数据预处理
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

from collections import Counter
import math

# data
def creat_data():
    iris = load_iris()
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df["label"] = iris.target
    df.columns = ["sepal length", "sepal width", "petal length","petal width", "label"]
    print(df.shape)
    data = np.array(df.iloc[:100, :])#加载数据集前100个　标签为0,1
    #print(data)
    return data[:, :-1], data[:, -1]

X, y = creat_data()
#随机分离数据集函数
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

#2.贝叶斯法
class NaiveBayes:
    
    def __init__(self):
        self.model = None
        
    #数学期望
    @staticmethod
    def mean(X):
        return sum(X)/float(len(X))
    
    #标准差
    def stdev(self, X):
        avg = self.mean(X)
        return math.sqrt(sum([pow(x-avg, 2) for x in X])/float(len(X)))
        
    #概率密度函数
    def gaussian_probability(self, x, mean, stdev):
        exponent = math.exp(-(math.pow(x- mean, 2)/(2*math.pow(stdev, 2))))
        
        return 1 / math.sqrt(2*math.pi*math.pow(stdev, 2))*exponent
    
    #处理X_train数据,得到列向量的均值和方差
    def summarize(self, train_data):
        #print(train_data)
#         for i in zip(*train_data):
#             print(i)
        summaries = [(self.mean(i), self.stdev(i)) for i in zip(*train_data)]#取出每一个数行的z
        #print(summaries)
        return summaries
    
    #分别求出期望和方差
    def fit(self, X, y):
        labels = list(set(y))#set()构建一个不重复的集合
        print("labels:",labels)
        #封装数据为为字典
        data = {label : [] for label in labels}
        for f, label in zip(X, y): 
            data[label].append(f)#添加数据字典
            
        self.model = {
            label: self.summarize(value) for label, value in data.items()
        }
        print("model:", self.model)
        return 'gaussian NB train done!'
    
    #计算概率
    def calculate_probabilities(self, input_data):
        probabilities = {}
        for label,value in self.model.items():
            probabilities[label]=1#保证非零
            print("value:", value)
            for i in range(len(value)):
                #print(i," : ",value[i])
                mean, stdev = value[i]
                probabilities[label] *= self.gaussian_probability(
                input_data[i], mean, stdev)
          
        print("可能:",probabilities)
        return probabilities

    # 类别
    def predict(self, X_test):
        # {0.0: 2.9680340789325763e-27, 1.0: 3.5749783019849535e-26}
        label = sorted(
            self.calculate_probabilities(X_test).items(),
            key=lambda x: x[-1])[-1][0]
        return label

    def score(self, X_test, y_test):
        right = 0
        for X, y in zip(X_test, y_test):
          
            label = self.predict(X)  
            print("label:",label)
            print("True:", y)
            if label == y:
                right += 1


                return right / float(len(X_test))
if __name__ == "__main__":
    model = NaiveBayes()
	model.fit(X_train, y_train)
	print(model.predict([4.4,  3.2,  1.3,  0.2]))
	model.score(X_test, y_test)
```

